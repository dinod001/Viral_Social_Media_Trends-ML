data:
  raw_path: "data/raw/Viral_Social_Media_Trends.csv"
  processed_dir: "data/processed"

ingestion:
  # DataIngestorCSV.ingest(file_path_or_link)
  file_type: "csv"
  source_path: "Piepeline/data/raw/Viral_Social_Media_Trends.csv"

missing_values:
  # DropMissingValuesStrategy.handle(df)
  strategy: "drop_missing_and_duplicates"

outliers:
  # OutlierDetector.handle_outliers(df, selected_columns)
  detection_method: "IQR"
  # Columns from EDA/03_handling_outliers.ipynb - all numerical columns used for outlier detection
  selected_columns:
    - Likes
    - Shares
    - Comments

  target_column: Views
  # Remove a row if it is an outlier in >= this many selected columns
  remove_if_outlier_count_ge: 2

feature_engineering:
  # NewFeatureEngineer.__init__(means_file), fit(df), handle(df)
  means_file: "platform_means.json"
  required_columns:
    - Platform
    - Views
    - Likes
    - Shares
    - Comments
  # Engineered outputs used in EDA (case as seen in processed CSVs)
  outputs:
    - Like_Rate
    - Share_Rate
    - Comment_Rate
    - Engagement_Rate
    - Like_to_Comment_Ratio
    - Share_to_Like_Ratio
    - Views_Norm
    # Note: EDA uses lowercase 'engagement_rate'

preprocessing:
  regression:
    # RegressionPreprocessor.__init__(...)
    save_path: "data/processed"
    artifacts_path: "artifacts/preprocessor/"
    # Drop per EDA 05_encoding_and_scalling.ipynb
    columns_to_drop:
      - Share_Rate
      - Total_Engagement
      - Engagement_Rate
      - Engagement_Level
    nominal_columns:
      - Platform
      - Hashtag
      - Content_Type
      - Region
    numerical_columns:
      - Views
      - Likes
      - Shares
      - Comments
      - Like_Rate
      - Comment_Rate
      - Total_Engagement_wo_Shares
      - Engagement_Rate_wo_Shares
      - Views_Norm
    ordinal_columns: 
      

  classification:
    # ClassificationPreprocessor.__init__(...)
    save_path: "Piepeline/data/processed"
    artifacts_path: "Piepeline/artifacts/encode"
    encoder_path: "artifacts/encoder"
    # From EDA/data/processed/df_cla.csv header
    columns_to_keep:
      - Platform
      - Hashtag
      - Content_Type
      - Region
      - Views
      - Views_Norm
      - Engagement_Level
    nominal_columns:
      - Platform
      - Hashtag
      - Content_Type
      - Region
    numerical_columns:
      - Views
      - Views_Norm
    target_column: Engagement_Level

splitting:
  # SimpleTrainTestSplitStrategy.__init__(test_size, random_state, stratify)
  # and split_data(df, target_column)
  regression:
    target_column: "Shares"
    test_size: 0.2
    random_state: 42
    stratify: false
    saving_path: "artifacts/data/regression/"
  classification:
    target_column: "Engagement_Level"
    test_size: 0.2
    random_state: 42
    stratify: true
    saving_path: "artifacts/data/classification/"

model_building:
  # Default parameters for model builders (BaseModelBuilder classes)
  classification:
    RandomForest:
      criterion: "entropy"
      max_depth: 10
      n_estimators: 100
      min_samples_split: 2
      min_samples_leaf: 1
      random_state: 42
    XGBoost:
      max_depth: 10
      n_estimators: 200
      learning_rate: 0.05
      random_state: 42
  regression:
    RandomForest:
      max_depth: 8
      n_estimators: 100
      min_samples_split: 2
      min_samples_leaf: 5
      random_state: 42
    XGBoost:
      learning_rate: 0.05
      max_depth: 6
      n_estimators: 100
      subsample: 0.8

model_training:
  # ModelTrainer settings
  cv: 5
  n_jobs: -1
  verbose: 1
  return_train_score: false
  
  classification:
    # Cross-validation settings
    stratified_kfold:
      n_splits: 6
      shuffle: true
      random_state: 42

    # Parameter grids for GridSearchCV
    param_grids:
      LogisticRegression:
        max_iter: [1000, 5000, 10000]
        penalty: ["l2", null]
        C: [0.1, 1, 10]
      RandomForest:
        n_estimators: [100]
        max_depth: [8, 12]
        criterion: ["gini", "entropy", "log_loss"]
      XGBoost:
        n_estimators: [100, 200]
        learning_rate: [0.05, 0.1, 0.2]
        max_depth: [6, 8, 10]
    # Model save directory
    model_dir: "artifacts/models/classification/cla_socialMedia_analyzer.joblib"
    # Class names for evaluation
    class_names: ["High", "Low", "Medium"]
  
  regression:
    # Cross-validation settings
    kfold:
      n_splits: 6
      shuffle: true
      random_state: 42

    # Parameter grids for GridSearchCV
    param_grids:
      LinearRegression:
        fit_intercept: [true, false]
        n_jobs: [1, 5, 10, 15, null]
        positive: [true, false]
      RandomForest:
        n_estimators: [100, 200]
        max_depth: [8, 12, 16]
        min_samples_leaf: [1, 2, 5]
      XGBoost:
        n_estimators: [100, 200]
        learning_rate: [0.05, 0.1, 0.2]
        max_depth: [6, 8, 10]
        subsample: [0.8, 1]
    # Model save directory
    model_dir: "artifacts/models/regression/reg_socialMedia_analyzer.joblib"

model_evaluation:
  # ModelEvaluator settings
  classification:
    # Metrics to compute
    metrics: ["accuracy", "precision", "recall", "f1", "confusion_matrix"]
    # Zero division handling
    zero_division: 0
    # Figure save directory
    figures_dir: "artifacts/figures/classification/"
  
  regression:
    # Metrics to compute
    metrics: ["r2", "mae", "mse", "rmse"]
    # Figure save directory
    figures_dir: "artifacts/figures/regression/"

